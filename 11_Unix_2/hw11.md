# Занятие №11 - Unix утилиты #2
### Работа с файловой системой

Добавить к ВМ три дополнительных диска небольшого размера (2-5 гигов хватит за глаза)
---
#### Примечание
 Если после перегрузки появился md127 вместо md1
 ```bash
 sdb       8:16   0     5G  0 disk
└─md127   9:127  0     5G  0 raid1
sdc       8:32   0     5G  0 disk
└─md127   9:127  0     5G  0 raid1

 ```
 Это по умолчанию почему то ставит ubunta
 В этом случае останови и пересобери
 ```bash
 sudo mdadm --stop /dev/md127
 sudo mdadm --assemble /dev/md1 /dev/sdb /dev/sdc 
 
 
lsblk 
 sdb      8:16   0     5G  0 disk
└─md1    9:1    0     5G  0 raid1
sdc      8:32   0     5G  0 disk
└─md1    9:1    0     5G  0 raid1

 ```
 ----
 
 Собрать из двух дополнительных дисков raid-1 (имя устройства задать /dev/md1)
```bash
 lsblk 

sda      8:0    0    50G  0 disk
├─sda1   8:1    0     1M  0 part
└─sda2   8:2    0    50G  0 part /
sdb      8:16   0     5G  0 disk
sdc      8:32   0     5G  0 disk
sdd      8:48   0     5G  0 disk

```

## создадим raid-1

```bash
sudo apt -y install mdadm

sudo mdadm --create --verbose /dev/md1 --level=1 --raid-devices=2 /dev/sdb /dev/sdc

# mdadm: array /dev/md1 started.
```
Читаем конфигурацию
```bash
sudo mdadm --detail --scan
ARRAY /dev/md1 metadata=1.2 UUID=19bdfac3:ade22332:f25752e7:b96346de

```

Пока стрелка [=====>...............] создаётся raid

```bash
cat /proc/mdstat
 cat /proc/mdstat
Personalities : [raid1]
md1 : active raid1 sdc[1] sdb[0]
      5237760 blocks super 1.2 [2/2] [UU]
      [=====>...............]  resync = 29.9% (1569024/5237760) finish=0.5min speed=120694K/sec
```

```bash
lsblk

sda      8:0    0    50G  0 disk
├─sda1   8:1    0     1M  0 part
└─sda2   8:2    0    50G  0 part  /
sdb      8:16   0     5G  0 disk
└─md1    9:1    0     5G  0 raid1
sdc      8:32   0     5G  0 disk
└─md1    9:1    0     5G  0 raid1

```
Инфо об устройстве
```bash
sudo mdadm --detail --scan --verbose
ARRAY /dev/md1 metadata=1.2 UUID=19bdfac3:ade22332:f25752e7:b96346de
	
```

Внесём в конфигурацию `/dev/md1`
```bash
sudo vim /etc/mdadm/mdadm.conf

# Добавим строку	
DEVICE partitions
ARRAY /dev/md1 level=raid1 num-devices=2 metadata=1.2 UUID=19bdfac3:ade22332:f25752e7:b96346de
	
```

Форматируем и создём файловую систему

```bash
sudo mkfs.ext4 /dev/md1


Creating filesystem with 1309440 4k blocks and 327680 inodes
Filesystem UUID: 319360f6-219b-4334-a1f0-bf4e5d8519c0
```

Создадим папку , которая будет служить диском
```bash

sudo mkdir /raid1

sudo vim /etc/fstab
# Добавил
UUID="319360f6-219b-4334-a1f0-bf4e5d8519c0" /raid1 ext4 defaults 0 2
```

Перечитаем сервисы и смонтируем (монтирование не нужно для следующего задания)

```bash
sudo systemctl daemon-reload
sudo mount -a
```

Смонтированные диски
```bash
df -h | grep -v tmp

Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        49G  5.9G   41G  13% /
/dev/md1        4.9G   24K  4.6G   1% /raid1
```


посмотреть из чего состоит raid

```bash
sudo mdadm --detail --verbose /dev/md1

/dev/md1:
           Version : 1.2
     Creation Time : Mon Jul 21 12:20:32 2025
        Raid Level : raid1
        Array Size : 5237760 (5.00 GiB 5.36 GB)
     Used Dev Size : 5237760 (5.00 GiB 5.36 GB)
      Raid Devices : 2
     Total Devices : 2
       Persistence : Superblock is persistent

       Update Time : Mon Jul 21 14:25:17 2025
             State : clean
    Active Devices : 2
   Working Devices : 2
    Failed Devices : 0
     Spare Devices : 0

Consistency Policy : resync

              Name : ubuntu1:1  (local to host ubuntu1)
              UUID : 19bdfac3:ade22332:f25752e7:b96346de
            Events : 17

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc


```
Посмотрели и для LVM его прийдётся размонтировать



## Из полученного массива /dev/md1 и третьего диска собрать volume group (lvm) с именем vg_raid

Прооверка используются ли устройства
```bash
sudo apt -y install lvm2

sudo pvscan
sudo vgscan
sudo lvscan

#  No matching physical volumes found
```


Делаем оба устройства как физические тома LVM

```
sudo umount /dev/md1

sudo pvcreate /dev/md1
# Physical volume "/dev/md1" successfully created.

sudo pvcreate /dev/sdd
# Physical volume "/dev/sdd" successfully created.

```


Создадим группу
```bash
sudo vgcreate vg_raid /dev/md1 /dev/sdd

Volume group "vg_raid" successfully created


Проверим
sudo vgs

  VG      #PV #LV #SN Attr   VSize  VFree
  vg_raid   2   0   0 wz--n- <9.99g <9.99g

sudo pvs
  PV         VG      Fmt  Attr PSize  PFree
  /dev/md1 vg_raid lvm2 a--   4.99g  4.99g
  /dev/sdd   vg_raid lvm2 a--  <5.00g <5.00g
```


```bash
sudo pvdisplay
  --- Physical volume ---
  PV Name               /dev/md1
  VG Name               vg_raid
  PV Size               <5.00 GiB / not usable 3.00 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              1278
  Free PE               1278
  Allocated PE          0
  PV UUID               BuE0Ta-cXMM-pddZ-s1xe-jDh0-dBUh-M9aBuw

  --- Physical volume ---
  PV Name               /dev/sdd
  VG Name               vg_raid
  PV Size               5.00 GiB / not usable 4.00 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              1279
  Free PE               1279
  Allocated PE          0
  PV UUID               CF9I8I-9Jit-Zfl3-ErVo-X4oS-PeD4-SCPoZ7

```
Свободно почти 10 гб
```bash
sudo vgdisplay vg_raid
  VG Name               vg_raid
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               <9.99 GiB
  PE Size               4.00 MiB
  Total PE              2557
  Alloc PE / Size       0 / 0
  Free  PE / Size       2557 / <9.99 GiB
  VG UUID               EgdWNU-7lvJ-qOPc-p6C8-1BSJ-8osU-bNz8Pz

```



## От созданной VG vg_raid отрезать logical volume размером 3Gb и именем mysql-app1; 

создать на разделе файловую систему xfs

```bash
sudo lvcreate -n mysql-app1 -L3G vg_raid
# Logical volume "mysql-app1" created.
```
Для файловой системы `xfs`
```bash
sudo apt -y install xfsprogs
sudo mkfs.xfs /dev/vg_raid/mysql-app1

meta-data=/dev/vg_raid/mysql-app1 isize=512    agcount=4, agsize=196608 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=1
         =                       reflink=1    bigtime=1 inobtcount=1 nrext64=0
data     =                       bsize=4096   blocks=786432, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=16384, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

```

Точка монтирования
```bash
sudo mkdir /mnt/mysql-app1
sudo mount /dev/vg_raid/mysql-app1 /mnt/mysql-app1
```
Добавим строку
```bash
sudo vim /etc/fstab
/dev/vg_raid/mysql-app1 /mnt/mysql-app1 xfs defaults 0 2
```
Проверить монтирование
```bash
df -h | grep mysql-app1
/dev/mapper/vg_raid-mysql--app1  3.0G   90M  2.9G   3% /mnt/mysql-app1

```

```bash
sudo lvdisplay vg_raid

  --- Logical volume ---
  LV Path                /dev/vg_raid/mysql-app1
  LV Name                mysql-app1
  VG Name                vg_raid
  LV UUID                t04l4k-KVV3-3xgC-9WLE-qyBi-Pjn9-zHl0PD
  LV Write Access        read/write
  LV Creation host, time ubuntu1, 2025-07-21 12:54:58 +0000
  LV Status              available
  # open                 1
  LV Size                3.00 GiB
  Current LE             768
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           252:0
```

```bash
lsblk
sda                       8:0    0  46.7G  0 disk
├─sda1                    8:1    0     1M  0 part
└─sda2                    8:2    0  46.7G  0 part  /
sdb                       8:16   0     5G  0 disk
└─md1                     9:1    0     5G  0 raid1
  └─vg_raid-mysql--app1 252:0    0     3G  0 lvm   /mnt/mysql-app1
sdc                       8:32   0     5G  0 disk
└─md1                     9:1    0     5G  0 raid1
  └─vg_raid-mysql--app1 252:0    0     3G  0 lvm   /mnt/mysql-app1
sdd                       8:48   0     5G  0 disk


```


## Установить на ВМ mariadb-server

https://linuxconfig.org/how-to-install-and-secure-mariadb-on-ubuntu-24-04

Установка сервера
```bash
sudo apt update 
sudo apt install -y mariadb-server 
```
Статус
```bash

sudo systemctl start mariadb
sudo systemctl status mariadb
sudo systemctl enable mariadb
```
Безопасные настройки  (внести пароль для root)
- для данного примера не ставлю
```bash
sudo mysql_secure_installation


All done!  If you've completed all of the above steps, your MariaDB
installation should now be secure.

```


#### Вынести папку с данными mariadb-server на созданный logical volume mysql-app1. 

```bash
sudo systemctl stop mariadb
sudo rsync -av /var/lib/mysql/ /mnt/mysql-app1/
```

На всякий сделать копию данных
```bash
sudo mv /var/lib/mysql /var/lib/mysql.bak
```

Создадим симлинк
```bash
sudo ln -s /mnt/mysql-app1 /var/lib/mysql

```

Обязательно проверить, что БД после всех манипуляций запустилась и жива
```bash
sudo systemctl start mariadb
sudo systemctl status mariadb
```
Проверить подключение к БД - что то не так в безопасности отметил, но работает
```bash
mysql -u root -p

Enter password:
ERROR 1698 (28000): Access denied for user 'root'@'localhost'
```

### Увеличить размер mysql-app1 до 4G

```bash
	sudo lvresize -L 4G /dev/vg_raid/mysql-app1
	
size of logical volume vg_raid/mysql-app1 changed from 3.00 GiB (768 extents) 
to 4.00 GiB (1024 extents).
  Logical volume vg_raid/mysql-app1 successfully resized.

df -h | grep mysql-app1
   /dev/mapper/vg_raid-mysql--app1  3.0G  215M  2.8G   8% /mnt/mysql-app1
	
	# расширим комнату
	sudo xfs_growfs /dev/vg_raid/mysql-app1
	
	
	 df -h | grep mysql-app1
/dev/mapper/vg_raid-mysql--app1  4.0G  235M  3.8G   6% /mnt/mysql-app1

```

### Оставшееся в volume group место отдать новому logical volume с именем app1 
и файловой системой ext4, который смонтировать в /opt/data-app1

Обращаем внимание на Free - те свободно почти 6 GB
```bash
sudo vgdisplay vg_raid

  --- Volume group ---
  VG Name               vg_raid
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  3
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                1
  Open LV               1
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               <9.99 GiB
  PE Size               4.00 MiB
  Total PE              2557
  Alloc PE / Size       1024 / 4.00 GiB
  Free  PE / Size       1533 / <5.99 GiB
  VG UUID               EgdWNU-7lvJ-qOPc-p6C8-1BSJ-8osU-bNz8Pz
```
Отдаём всё место новому logical volume с именем app1
```bash
sudo lvcreate -l 100%FREE vg_raid -n app1

# Logical volume "app1" created.
```
Создадим файловую систему
```bash
sudo mkfs.ext4 /dev/vg_raid/app1

Creating filesystem with 1569792 4k blocks and 392448 inodes
Filesystem UUID: f2e9324e-22a3-403d-be8b-c9a6a55db9ac
```

Создаю точку монтирования и смонтирую
```bash
sudo mkdir -p /opt/data-app1
sudo mount /dev/vg_raid/app1 /opt/data-app1
```
Добавим в автозагрузку 
```bash
sudo vim /etc/fstab
/dev/vg_raid/app1 /opt/data-app1 ext4 defaults 0 2
```

Проверим
```bash
df -h | grep -v tmp

Filesystem                       Size  Used Avail Use% Mounted on
/dev/sda2                         46G   15G   29G  35% /
/dev/mapper/vg_raid-mysql--app1  4.0G  235M  3.8G   6% /mnt/mysql-app1
/dev/mapper/vg_raid-app1         5.9G   24K  5.5G   1% /opt/data-app1

```

## Сымитировать сбой одного из дисков в raid массиве; 
проверить, что данные не побились из-за этого; 

```bash
lsblk

sda                       8:0    0  46.7G  0 disk
├─sda1                    8:1    0     1M  0 part
└─sda2                    8:2    0  46.7G  0 part  /
sdb                       8:16   0     5G  0 disk
└─md1                     9:1    0     5G  0 raid1
  ├─vg_raid-mysql--app1 252:0    0     4G  0 lvm   /mnt/mysql-app1
  └─vg_raid-app1        252:1    0     6G  0 lvm   /opt/data-app1
sdc                       8:32   0     5G  0 disk
└─md1                     9:1    0     5G  0 raid1
  ├─vg_raid-mysql--app1 252:0    0     4G  0 lvm   /mnt/mysql-app1
  └─vg_raid-app1        252:1    0     6G  0 lvm   /opt/data-app1
sdd                       8:48   0     5G  0 disk
└─vg_raid-app1          252:1    0     6G  0 lvm   /opt/data-app1


sudo mdadm --detail /dev/md1


    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc


```

Во второй консоли запускаем
```bash
watch cat /proc/mdstat
```

Ломаем `/dev/sdb`
```bash
sudo mdadm /dev/md1 --fail /dev/sdb

# mdadm: set /dev/sdb faulty in /dev/md1
```
 во второй консоли появлась `F` рядом с диском `sdb[0]`
 ```bash
 Personalities : [raid1] [linear] [raid0] [raid6] [raid5] [raid4] [raid10]
md1 : active raid1 sdb[0](F) sdc[1]
      5237760 blocks super 1.2 [2/1] [_U] 
```

### удалить сбойный диск из массива; 
```bash
sudo mdadm /dev/md1 --remove /dev/sdb
# mdadm: hot removed /dev/sdb from /dev/md1

lsblk

sda                       8:0    0  46.7G  0 disk
├─sda1                    8:1    0     1M  0 part
└─sda2                    8:2    0  46.7G  0 part  /
sdb                       8:16   0     5G  0 disk
sdc                       8:32   0     5G  0 disk
└─md1                     9:1    0     5G  0 raid1
  ├─vg_raid-mysql--app1 252:0    0     4G  0 lvm   /mnt/mysql-app1
  └─vg_raid-app1        252:1    0     6G  0 lvm   /opt/data-app1
sdd                       8:48   0     5G  0 disk
└─vg_raid-app1          252:1    0     6G  0 lvm   /opt/data-app1
```

Ни куда диски не делись смонтированные - всё работает
```bash
df -h | grep -v tmp

Filesystem                       Size  Used Avail Use% Mounted on
/dev/sda2                         46G   15G   29G  35% /
/dev/mapper/vg_raid-mysql--app1  4.0G  235M  3.8G   6% /mnt/mysql-app1
/dev/mapper/vg_raid-app1         5.9G   24K  5.5G   1% /opt/data-app1
```
2 консоль - остался один диск
```bash
cat /proc/mdstat
Personalities : [raid1] [linear] [raid0] [raid6] [raid5] [raid4] [raid10]
md1 : active raid1 sdc[1]
```


### вернуть обратно
```bash
sudo mdadm /dev/md1 --add /dev/sdb

# mdadm: added /dev/sdb

```
2 консоль - процесс соединения

```bash
cat /proc/mdstat

Personalities : [raid1] [linear] [raid0] [raid6] [raid5] [raid4] [raid10]
md1 : active raid1 sdb[2] sdc[1]
      5237760 blocks super 1.2 [2/1] [_U]
      [=============>.......] 
```


### Попробовать удалить диск из raid-массива 
без метки --fail. Что произошло (или могло бы произойти)?

пробую ломать
```bash
sudo mdadm /dev/md1 --remove /dev/sdb

# mdadm: hot remove failed for /dev/sdb: Device or resource busy
```
Диск живой и используется...



## Работа с пользователями и правами доступа
### Добавить в систему пользователей vmadmin и vmuser; 
группу ssh-users
```bash
sudo useradd vmadmin 
sudo useradd vmuser

sudo cat /etc/passwd | grep vm

vmadmin:x:1002:1002::/home/vmadmin:/bin/sh
vmuser:x:1003:1003::/home/vmuser:/bin/sh
```
```bash
sudo useradd vmadmin
sudo ls /home/vmadmin
ls: cannot access '/home/vmadmin': No such file or directory

```

```bash
sudo adduser vmadmin
sudo ls -l /home/vmadmin
# total 0
```

useradd - без создания home директории пользователя
adduser - создаёт home/user с указанием пароля в интерактивном

Для удаления можно использовать

```bash
sudo deluser --remove-home vmadmin
```

#### Группа
```bash
sudo groupadd ssh-users
```

проверка существования группы
```bash
cut -d: -f1 /etc/group | grep ssh
# ssh-users

getent group ssh-users
# ssh-users:x:1004:
```

### Добавить пользователей vmadmin и vmuser в группу ssh-users

проверим до 
```bash
groups vmadmin
# vmadmin : vmadmin

groups vmuser
# vmuser : vmuser
```

Добавим в группу ssh-users
```bash

sudo usermod -aG ssh-users vmadmin
groups vmadmin

# vmadmin : vmadmin ssh-users

sudo usermod -aG ssh-users  vmuser
groups vmuser

#vmuser : vmuser ssh-users

```

Удалить из группы
```bash
deluser vmadmin ssh-users
#info: Removing user `vmadmin' from group `ssh-users' ...
```

Кто в группе
```bash
getent group ssh-users
# ssh-users:x:1004:vmadmin,vmuser
```

Удалить группу
```bash
sudo groupdel ssh-users
```

Если группа используется как основная
 `groupdel: cannot remove the primary group of user 'vmadmin'`
 основную группу пользователя изменить
```bash
sudo usermod -g users vmadmin
```

### Пользователь vmadmin должен иметь полные права sudo без пароля
```bash
sudo visudo

# User privilege specification
vmadmin ALL=(ALL) NOPASSWD:ALL

#Проверка ошибок в файле
sudo visudo -c

/etc/sudoers: parsed OK
/etc/sudoers.d/README: parsed OK

```
Зайдём под vmadmin и проверим
```bash
 su - vmadmin
Password:
vmadmin@ubuntu1:~$ systemctl status nginx
● nginx.service - A high performance web server and a reverse proxy server
     Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; preset: enabled)
     Active: active (running) since Tue 2025-07-22 07:37:39 UTC; 2h 40min ago

```
Да - запустилось без `sudo`


Или запустил новую сессию
```bash
ssh vmadmin@192.168.50.34 -p 1234
vmadmin@192.168.50.34's password:

sudo systemctl restart nginx
echo $?
0
```
отработало не запрашивая пароль для sudo

для vmuser
```bash
sudo systemctl restart nginx
[sudo] password for vmuser:
vmuser is not in the sudoers file.

```

### Пользователь vmuser должен иметь возможность запускать от имени root только команды
apt list --installed
top/htop
strace

```bash
sudo visudo

# Cmnd alias specification
Cmnd_Alias WEB_CMDS = /usr/bin/apt list --installed, /usr/bin/top, /usr/bin/htop, /usr/bin/strace


# User privilege specification
vmuser ALL=(root) NOPASSWD: WEB_CMDS
```
`sudo visudo -c`



### Для всех пользователей из группы ssh-users должен быть запрещен вход по ssh 
в систему по выходным (суббота и воскресенье)

Модуль PAM (pam_time) позволяет настраивать временные ограничения 
для пользователей или групп. 
Это наиболее гибкий способ ограничить доступ по дням недели.


```bash
sudo vim /etc/security/time.conf
# добавить строку
ssh;*;@ssh-users;!Wk1700-We0900
```
пояснение
`ssh;*;@ssh-users;!Wk1700-We0900`

`ssh`: Указывает, что правило применяется к сервису SSH.
`*`: Применяется ко всем терминалам.
`@ssh-users`: Ограничение применяется к группе ssh-users.
`!Wk1700-We0900`: Запрещает доступ в выходные дни (суббота и воскресенье).
`Wk` — начало рабочей недели (понедельник).
`We` — конец рабочей недели (пятница).
`!` — отрицание (запрет).
`Wk1700-We0900` — время работы с понедельника 17:00 до пятницы 09:00.

настройка PAM
```bash
sudo vim /etc/pam.d/sshd
# если нет в файле строки - добавим
account required pam_time.so
```
Перезапустим службу SSH
```bash
sudo systemctl restart ssh
```
Проверка
```bash
date
#Tue Jul 22 01:17:54 PM UTC 2025
Поменял на машине в графическом интерфейсе на 20 число - воскресенье

date
#Sun Jul 20 01:25:00 PM UTC 2025


```
????  Почему заходит по ssh 
```bash
ssh vmadmin@192.168.50.34 -p 1234
vmadmin@192.168.50.34's password:
Welcome to Ubuntu 24.04.2 LTS (GNU/Linux 6.14.0-24-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

Expanded Security Maintenance for Applications is not enabled.

73 updates can be applied immediately.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status


```

### Всем пользователям из группы ssh-users ограничить количество одновременных подключений 2
смотрим количество сессий
```bash
alrex@ubuntu1:~$ who | awk '{print $1}' | sort | uniq -c
      1 alrex
      2 vmuser

```

Через PAM
```bash
sudo vim /etc/security/limits.conf

#перед end
@ssh-users hard maxlogins 2

```
настройка PAM

```bash
sudo vim /etc/pam.d/common-session
# если нет в файле строки - добавим
session required pam_limits.so
```

Перезапустим службу SSH
```bash
sudo systemctl restart ssh
```
Проверим - подключимся 3й сессией

```bash
$ ssh vmuser@192.168.50.34 -p 1234
vmuser@192.168.50.34's password:
There were too many logins for 'vmuser'.
Welcome to Ubuntu 24.04.2 LTS (GNU/Linux 6.14.0-24-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

There were too many logins for 'vmuser'.
Last login: Tue Jul 22 13:04:27 2025 from 192.168.50.108
Connection to 192.168.50.34 closed.
```
Отработало - 3 закрыло.


